{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): \\ DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "| DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-64/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-64/current_repodata.json HTTP/1.1\" 304 0\n",
      "\\ DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "# load and set out key\n",
    "openai.api_key = open('key.txt','r').read().strip('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harnessing Function Calling with OpenAI API for GPT-4 and GPT-3.5\n",
    "\n",
    "Explore this narrative through the lens of a notebook: link\n",
    "\n",
    "OpenAI has unveiled a game-changing feature for their models via the API, dubbed \"Function Calling.\" This innovation aims to significantly simplify the process of deriving structured, deterministic information from an inherently unstructured and non-deterministic behemoth like GPT-4.\n",
    "\n",
    "The endeavor of molding and extracting deterministic outputs from a language model has historically been a Herculean task, sparking a plethora of research. The conventional tactic has been a relentless trial and error with various pre-prompts and few-shot learning examples until stumbling upon a somewhat working solution. Although feasible, the outcome was often cumbersome and lacked reliability. Fast forward to now, the function calling feature paves the way for crafting robust programs, essentially infusing intelligence into your applications.\n",
    "\n",
    "Picture a scenario where you wish to smartly manage a myriad of input types, including queries like: \"What's the weather like in Boston?\"\n",
    "\n",
    "The challenge, when faced with such natural language input for GPT-4, unfolds as follows:\n",
    "\n",
    "Discerning if the user is in pursuit of weather information.\n",
    "Upon affirmation, extracting the specified location from their input.\n",
    "For instance, if the user greets with \"Hello, how are you today?\", there's no need to trigger the function or attempt to extract a location.\n",
    "\n",
    "However, when the user inquires, \"What's the weather like in Boston?\", it's imperative to recognize the intent to fetch weather details and extract the location \"Boston\" from the query.\n",
    "\n",
    "In the bygone era, you might have channeled this input to the OpenAI API for GPT-4 in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    messages = [{\"role\":\"user\",\"content\":\"Waht's the weather like in Cardiff UK?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you'd access the response via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI language model, I don't have access to real-time data. However, Cardiff generally has a maritime climate with mild winters and relatively cool summers. It often experiences frequent rainfall throughout the year. To get the most accurate and up-to-date weather information for Cardiff, I recommend checking a reliable weather website or using a weather app.\n"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "print(reply_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this isn't quite what we would want to happen in this scenario! While GPT-4 may not currently be able to access the internet for us, we could conceivably do this ourselves, but we still would need to identify the intent, as well as the particular desired location. Imagine we have a function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location,unit=\"fahrenheit\"):\n",
    "    #get the current weather in a given location\n",
    "    weather_info = {\n",
    "        \"location\":location,\n",
    "        \"unit\":unit,\n",
    "        \"temperature\": 20,\n",
    "        \"humidity\": 80,\n",
    "        \"wind\": 10,\n",
    "        \"rain\": 0,\n",
    "        \"snow\": 0,\n",
    "        \"forecast\":[\"sunny\",\"windy\"],\n",
    "\n",
    "    }\n",
    "    return weather_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a placeholder type of function to show how this all ties together, but it could be anything here. Extracting the intent and location could be done with a preprompt, and this is sort of how OpenAI is doing it through the API, but the model has been trained for us with a particular structure, so we can use this to save a lot of R&D time to find the \"best prompt\" to get it done.\n",
    "\n",
    "To do this, we want to make sure that we're using version `gpt-4-0613` or later. Then, we can pass a new functions parameter to the `ChatCompletion` call like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0613\"\n",
    "messages = [{\"role\":\"user\",\"content\":\"What's the weather like in Cardiff UK?\"}]\n",
    "functions = [\n",
    "        {\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"description\":\"Get the current weather in given location\",\n",
    "         \"parameters\":{\n",
    "             \"type\":\"object\",\n",
    "             \"properties\":{\n",
    "                 \"location\":{\n",
    "                     \"type\":\"string\",\n",
    "                     \"description\":\"the city and state, e.g. San Francisco, CA\",\n",
    "                 },\n",
    "                 \"unit\":{\n",
    "                     \"type\":\"string\",\n",
    "                     \"description\":\"the unit of temperature\",\n",
    "                     \"enum\":[\"celsius\",\"fahrenheit\"]},\n",
    "             },\n",
    "             \"required\":[\"location\"],\n",
    "         },\n",
    "         }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    functions = functions,\n",
    "    function_call= \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is the `function_call=\"auto\"`, part. This will let GPT-4 determine if it should seek to fulfill the function parameters. You can also set it to none to force no function to be detected, and finally you can set it to seek parameters for a specific function by doing something like `function_call={\"name\": \"get_current_weather\"}`. There are many instances where it could be hard for GPT-4 to determine if a function should be run, so being able to force it to run if you know it should be is actually very powerful, which I'll show soon.\n",
    "\n",
    "Beyond this, we name and describe the function, then describe the parameters that we'd hope to pass to this function. GPT-4 is relying on this description to help identify what it is you want, so try to be as clear as possible here. The API is going to return to you a json structured object, and this is how you structure your function description, which affords you quite a bit of flexibility in how you describe/structure this functionality.\n",
    "\n",
    "Okay let's see how GPT-4 responds to our new prompt:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
